{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0  22 118  24   0   0   0   0   0  48\n",
      "   88   5   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  12 100 212 205 185 179 173 186 193 221\n",
      "  142  85   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  85  76 199 225 248 255 238 226 157\n",
      "   68  80   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  91  69  91 201 218 225 209 158  61\n",
      "   93  72   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  79  89  61  59  87 108  75  56  76\n",
      "   97  73   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  75  89  80  80  67  63  73  83  80\n",
      "   96  72   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  77  88  77  80  83  83  83  83  81\n",
      "   95  76   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  89  96  80  83  81  84  85  85  85\n",
      "   97  84   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  93  97  81  85  84  85  87  88  84\n",
      "   99  87   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  95  87  84  87  88  85  87  87  84\n",
      "   92  87   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  97  87  87  85  88  87  87  87  88\n",
      "   85 107   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  17 100  88  87  87  88  87  87  85  89\n",
      "   77 118   8   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10  93  87  87  87  87  87  88  87  89\n",
      "   80 103   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   9  96  87  87  87  87  87  88  87  88\n",
      "   87 103   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  12  96  85  87  87  87  85  87  87  88\n",
      "   89 100   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  20  95  84  88  85  87  88  88  88  89\n",
      "   88  99   8   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  21  96  85  87  85  88  88  88  88  89\n",
      "   89  99  10   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  24  96  85  87  85  87  88  88  89  88\n",
      "   91 102  14   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  25  93  84  88  87  87  87  87  87  89\n",
      "   91 103  29   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  95  85  88  88  87  87  87  87  89\n",
      "   88 102  37   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  34  96  88  87  87  87  87  87  87  85\n",
      "   85  97  38   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  40  96  87  85  87  87  87  87  87  85\n",
      "   84  92  49   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  46  95  83  84  87  87  87  87  87  87\n",
      "   84  87  84   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  72  95  85  84  85  88  87  87  89  87\n",
      "   85  83  63   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  64 100  84  87  88  85  88  88  84  87\n",
      "   83  95  53   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  10 102 100  91  91  89  85  84  84  87\n",
      "  108 106  14   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8  73  93 104 107 103 103 106 102\n",
      "   75  10   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0  18  42  57  56  32   8\n",
      "    0   0   1   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20e7001db70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[8],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.00124563,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.02412748,\n",
       "        0.09909212, 0.20828982, 0.19343336, 0.14926102, 0.14223356,\n",
       "        0.15972377, 0.22432495, 0.16124382, 0.01016924, 0.        ,\n",
       "        0.        , 0.        , 0.00102468, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.04248593, 0.        ,\n",
       "        0.        , 0.02130536, 0.21904058, 0.23053401, 0.22592093,\n",
       "        0.22070519, 0.22380077, 0.27215623, 0.28834515, 0.29015647,\n",
       "        0.28319818, 0.25025195, 0.2221331 , 0.23276257, 0.2008961 ,\n",
       "        0.20302441, 0.02081872, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.04248593, 0.        ,\n",
       "        0.03697042, 0.22796732, 0.31936451, 0.21199862, 0.19521323,\n",
       "        0.1970582 , 0.18613133, 0.16869188, 0.18318398, 0.18092109,\n",
       "        0.17218449, 0.17810724, 0.20183667, 0.20677451, 0.20196469,\n",
       "        0.31420445, 0.22655668, 0.08402354, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17957063, 0.20240089, 0.28759526, 0.20504784, 0.19301982,\n",
       "        0.19255402, 0.18723926, 0.18218723, 0.17526862, 0.16840454,\n",
       "        0.17445008, 0.19050712, 0.19619878, 0.19773519, 0.18700434,\n",
       "        0.28520096, 0.22410741, 0.19263933, 0.01311036, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.03114079,\n",
       "        0.20492178, 0.19174821, 0.29762765, 0.20157245, 0.20179345,\n",
       "        0.21057077, 0.20939775, 0.21030254, 0.20806081, 0.2059542 ,\n",
       "        0.21409782, 0.22545221, 0.2221331 , 0.21807367, 0.20303329,\n",
       "        0.28681227, 0.21431037, 0.1987874 , 0.09832772, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.05231652,\n",
       "        0.23027293, 0.20346616, 0.3293969 , 0.24096018, 0.22372752,\n",
       "        0.23759589, 0.23155624, 0.23616863, 0.23972224, 0.24009026,\n",
       "        0.24241764, 0.24236112, 0.24017436, 0.24180189, 0.22547381,\n",
       "        0.33998533, 0.23390446, 0.20493546, 0.17261977, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.10961556,\n",
       "        0.23344182, 0.22903258, 0.36283821, 0.25370326, 0.23140445,\n",
       "        0.20831867, 0.16618869, 0.13270428, 0.12099189, 0.11264898,\n",
       "        0.09968576, 0.09356267, 0.10148213, 0.15253858, 0.22654241,\n",
       "        0.32709489, 0.25349855, 0.22440433, 0.1846376 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02852004, 0.1257016 , 0.27087461, 0.0463385 , 0.        ,\n",
       "        0.        , 0.        , 0.01124613, 0.02148454, 0.03186032,\n",
       "        0.04417892, 0.05298127, 0.04059285, 0.        , 0.        ,\n",
       "        0.32709489, 0.28166506, 0.22542901, 0.22178363, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.17189713,\n",
       "        0.1436565 , 0.07563402, 0.11537252, 0.06255697, 0.23688796,\n",
       "        0.24435217, 0.22490869, 0.20692871, 0.18996857, 0.18547257,\n",
       "        0.18351242, 0.18374355, 0.20070909, 0.2497113 , 0.1987589 ,\n",
       "        0.06122959, 0.0318404 , 0.00717274, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.0834573 ,\n",
       "        0.14154391, 0.16405125, 0.37454267, 0.14944165, 0.07238243,\n",
       "        0.0912098 , 0.12962718, 0.14507502, 0.14473796, 0.15019864,\n",
       "        0.1551926 , 0.1476712 , 0.14545771, 0.09717272, 0.07800753,\n",
       "        0.25297486, 0.18491923, 0.13730676, 0.23598652, 0.05197149,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.25286318,\n",
       "        0.20914697, 0.18322607, 0.30598798, 0.23864325, 0.2796594 ,\n",
       "        0.28714195, 0.27698115, 0.27328085, 0.27138367, 0.27195058,\n",
       "        0.26620629, 0.26828813, 0.27512932, 0.28812842, 0.25432591,\n",
       "        0.2964801 , 0.19594091, 0.08812225, 0.10706796, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.15196703,\n",
       "        0.19858399, 0.23861999, 0.25248189, 0.12163855, 0.13928135,\n",
       "        0.10922655, 0.11079246, 0.11808432, 0.12890724, 0.13313062,\n",
       "        0.13253675, 0.1273805 , 0.11614066, 0.11073171, 0.11861418,\n",
       "        0.22880529, 0.31105619, 0.19571336, 0.2785952 , 0.14147794,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.20303792,\n",
       "        0.18907731, 0.21305357, 0.15884622, 0.17840321, 0.2171473 ,\n",
       "        0.22183123, 0.22158492, 0.22492251, 0.22389153, 0.2241601 ,\n",
       "        0.22429296, 0.22432495, 0.22777099, 0.22598307, 0.18807294,\n",
       "        0.13857222, 0.25227392, 0.16087434, 0.17698989, 0.02887305,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24538939,\n",
       "        0.21231586, 0.24394633, 0.11871665, 0.16681858, 0.21276049,\n",
       "        0.20381448, 0.2027502 , 0.20130564, 0.20579928, 0.20481633,\n",
       "        0.2027699 , 0.20290699, 0.21424004, 0.20903434, 0.21051346,\n",
       "        0.12245917, 0.26819412, 0.1895653 , 0.21959857, 0.09816836,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24788065,\n",
       "        0.20386548, 0.24075053, 0.0969798 , 0.17840321, 0.21056708,\n",
       "        0.20719262, 0.2071819 , 0.20692871, 0.21032234, 0.20936781,\n",
       "        0.20956665, 0.20628877, 0.2164952 , 0.21581384, 0.21371925,\n",
       "        0.09023307, 0.26819412, 0.20800949, 0.22615375, 0.17323829,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.25037191,\n",
       "        0.20492178, 0.23861999, 0.06855468, 0.18882937, 0.20837367,\n",
       "        0.20944472, 0.20607398, 0.20692871, 0.20919158, 0.20822994,\n",
       "        0.20956665, 0.20065247, 0.21424004, 0.21920358, 0.21585644,\n",
       "        0.05317306, 0.25839707, 0.20493546, 0.22506122, 0.21077325,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.25037191,\n",
       "        0.20809067, 0.23648946, 0.02842511, 0.19925553, 0.20837367,\n",
       "        0.20944472, 0.2071819 , 0.20467948, 0.21032234, 0.21050568,\n",
       "        0.21183224, 0.20290699, 0.21085731, 0.21807367, 0.21585644,\n",
       "        0.04189393, 0.2596217 , 0.20698481, 0.22178363, 0.21943516,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24912628,\n",
       "        0.20809067, 0.23755473, 0.        , 0.20504784, 0.20727697,\n",
       "        0.20719262, 0.20496605, 0.20018103, 0.20806081, 0.20822994,\n",
       "        0.20843386, 0.20290699, 0.20634699, 0.21355401, 0.21692504,\n",
       "        0.05639567, 0.24002761, 0.20800949, 0.22178363, 0.2425336 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24912628,\n",
       "        0.20809067, 0.23755473, 0.        , 0.21431554, 0.20508356,\n",
       "        0.20831867, 0.2071819 , 0.20243026, 0.20806081, 0.20709207,\n",
       "        0.20730107, 0.20065247, 0.20521941, 0.20677451, 0.21906223,\n",
       "        0.07089741, 0.19471628, 0.2121082 , 0.21959857, 0.24542091,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.23293307,\n",
       "        0.20914697, 0.23968526, 0.        , 0.2247417 , 0.20618026,\n",
       "        0.20719262, 0.20496605, 0.20243026, 0.20693005, 0.20822994,\n",
       "        0.20843386, 0.20403425, 0.20409183, 0.19999502, 0.22013083,\n",
       "        0.07412002, 0.15797736, 0.21620691, 0.21850604, 0.25408282,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.25491557, 0.23168744,\n",
       "        0.21125956, 0.22477151, 0.        , 0.23053401, 0.20727697,\n",
       "        0.20719262, 0.20385813, 0.20805332, 0.20579928, 0.20822994,\n",
       "        0.20843386, 0.20854329, 0.20521941, 0.19773519, 0.21906223,\n",
       "        0.08056524, 0.11878918, 0.2213303 , 0.21522845, 0.26851935,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.21242964, 0.23044181,\n",
       "        0.21548476, 0.19600928, 0.        , 0.2340094 , 0.20618026,\n",
       "        0.20494053, 0.20164228, 0.20580409, 0.20693005, 0.20936781,\n",
       "        0.20616827, 0.20290699, 0.20521941, 0.19660527, 0.21585644,\n",
       "        0.10151221, 0.07225321, 0.22542901, 0.21413592, 0.27140665,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.21242964, 0.22919618,\n",
       "        0.21759735, 0.16724705, 0.        , 0.23632633, 0.20508356,\n",
       "        0.21057077, 0.20939775, 0.21592561, 0.2148454 , 0.21619502,\n",
       "        0.21636341, 0.2141796 , 0.21085731, 0.20677451, 0.21585644,\n",
       "        0.12568178, 0.04286207, 0.22747836, 0.21522845, 0.27429396,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.21242964, 0.22795055,\n",
       "        0.21970995, 0.13528901, 0.        , 0.22821709, 0.18205279,\n",
       "        0.17228517, 0.16508076, 0.16756727, 0.16509173, 0.16840454,\n",
       "        0.16878611, 0.16908916, 0.17026446, 0.17852663, 0.20410188,\n",
       "        0.14501744, 0.00979705, 0.22850304, 0.21304339, 0.28584318,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.25491557, 0.22919618,\n",
       "        0.21970995, 0.12144053, 0.        , 0.23632633, 0.18972971,\n",
       "        0.18129355, 0.19942643, 0.19793181, 0.19449163, 0.19685125,\n",
       "        0.19597314, 0.19614342, 0.19845394, 0.18304629, 0.21585644,\n",
       "        0.18530006, 0.        , 0.2346511 , 0.21741351, 0.303167  ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.38237336, 0.22172239,\n",
       "        0.21548476, 0.1225058 , 0.        , 0.14017395, 0.14805498,\n",
       "        0.12836934, 0.12962718, 0.12820583, 0.12890724, 0.13313062,\n",
       "        0.13366954, 0.13414406, 0.13192676, 0.12768044, 0.15708365,\n",
       "        0.10151221, 0.        , 0.23055239, 0.21413592, 0.30894161,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.76474672, 0.22421365,\n",
       "        0.21759735, 0.13955009, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22952771, 0.21522845, 0.35513849,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.17563403,\n",
       "        0.15950097, 0.08096036, 0.        , 0.00115846, 0.0010967 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13628208, 0.18245254, 0.21077325,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan Kumar Yadalla\\Anaconda3\\envs\\day6.2\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=784, units=64, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pavan Kumar Yadalla\\Anaconda3\\envs\\day6.2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 64, init = 'uniform', activation = 'relu', input_dim =784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan Kumar Yadalla\\Anaconda3\\envs\\day6.2\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=64, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 64, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan Kumar Yadalla\\Anaconda3\\envs\\day6.2\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavan Kumar Yadalla\\Anaconda3\\envs\\day6.2\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape(x_train.shape[0],784) #Ann takes only 2 dimension(28*28=784) x_train.shape[0]=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test.reshape(x_test.shape[0],784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.arange(10) == y_train[:, None]\n",
    "y_test = np.arange(10) == y_test[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pavan Kumar Yadalla\\Anaconda3\\envs\\day6.2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 1.8502 - acc: 0.3703\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 1.3957 - acc: 0.3939\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 1.2575 - acc: 0.3976\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 1.1881 - acc: 0.4120\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 1.1389 - acc: 0.4355\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 1.0827 - acc: 0.5021\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 1.0337 - acc: 0.5483\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.9445 - acc: 0.6285\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.8002 - acc: 0.6790\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7154 - acc: 0.7361\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.6461 - acc: 0.7875\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.5626 - acc: 0.8176\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.4857 - acc: 0.8366\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.4510 - acc: 0.8506: 0s - loss: 0.4531 - \n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.4043 - acc: 0.8735\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3567 - acc: 0.8873: 0s - loss: 0.3586 - acc\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.3306 - acc: 0.8954\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3101 - acc: 0.9015\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2996 - acc: 0.9033\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2856 - acc: 0.9066\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2773 - acc: 0.9095: 0s - loss: 0.2760 - acc\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2681 - acc: 0.9113\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2581 - acc: 0.9147\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2507 - acc: 0.9167\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2460 - acc: 0.9193\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2368 - acc: 0.9207\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.2333 - acc: 0.9214\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.2266 - acc: 0.9237\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.2226 - acc: 0.9261\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2162 - acc: 0.9282\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2127 - acc: 0.9287\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2102 - acc: 0.9301\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2033 - acc: 0.9315\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2014 - acc: 0.9324\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.1952 - acc: 0.9347\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.1934 - acc: 0.9345\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.1867 - acc: 0.9381\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.1864 - acc: 0.9371\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.1818 - acc: 0.9395\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1789 - acc: 0.9416\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.1768 - acc: 0.9407\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.1733 - acc: 0.9422\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.1712 - acc: 0.9432\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1669 - acc: 0.9443\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.1666 - acc: 0.9447\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1637 - acc: 0.9462\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.1581 - acc: 0.9474\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.1576 - acc: 0.9472\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.1588 - acc: 0.9475\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.1531 - acc: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e1a5c4908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x_train,y_train,epochs=50,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 3s 53us/step\n",
      "Test accuracy: 0.9487666666666666\n",
      "Test loss 0.1546572476471464\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = classifier.evaluate(x_train,y_train)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss',test_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
